# ğŸš€ 21 Days of AI & Data Science Projects

Welcome to **21 Days of AI & Data Science Projects**! This comprehensive repository contains a curated collection of daily projects designed to enhance your programming skills, creativity, and problem-solving abilities in the fields of Artificial Intelligence, Machine Learning, and Data Science.

Each day features a unique, hands-on project complete with detailed notebooks, datasets, and comprehensive documentation. Whether you're a beginner or an experienced developer, these projects will help you learn new concepts, build practical skills, and create an impressive portfolio.

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [ğŸ“š Daily Projects](#-daily-projects)
- [ğŸ› ï¸ Technologies Used](#ï¸-technologies-used)
- [ğŸ“Š Project Categories](#-project-categories)
- [ğŸš€ Getting Started](#-getting-started)
- [ğŸ“ Repository Structure](#-repository-structure)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸ¯ Project Overview

This repository contains **21 comprehensive projects** spanning across multiple domains of AI and Data Science:

- **ğŸ“Š Data Analysis & Visualization** - Exploratory Data Analysis, Statistical Analysis
- **ğŸ¤– Machine Learning** - Classification, Regression, Clustering, Time Series
- **ğŸ§  Deep Learning** - Neural Networks, Computer Vision, Natural Language Processing
- **ğŸ¨ AI Applications** - Style Transfer, Object Detection, Text Generation
- **ğŸ”§ Advanced Tools** - Hugging Face Pipelines, Custom GPT, OCR, Search Engines

Each project is designed to be **self-contained** with clear objectives, step-by-step implementations, and real-world applications.

---

## ğŸ“š Daily Projects

### **Day 1: Data Storytelling - Analyzing Survival on the Titanic** ğŸš¢
**ğŸ“ Folder:** `Day 1/`  
**ğŸ““ Notebook:** `1_Data_Storytelling_Analysing_Survival_on_the_Titanic.ipynb`  
**ğŸ¯ Objective:** Comprehensive Exploratory Data Analysis (EDA) to understand key factors influencing survival on the Titanic  
**ğŸ”§ Skills:** Data cleaning, visualization, statistical analysis, feature engineering  
**ğŸ“Š Dataset:** Titanic passenger data  

### **Day 2: Netflix Content Strategy Analysis** ğŸ¬
**ğŸ“ Folder:** `Day 2/`  
**ğŸ““ Notebook:** `2_Cracking_the_Code_An_Inside_Look_at_Netflix's_Content_Strategy.ipynb`  
**ğŸ¯ Objective:** In-depth EDA of Netflix dataset exploring content trends, genres, and production patterns  
**ğŸ”§ Skills:** Time-series analysis, text data manipulation, advanced visualization  
**ğŸ“Š Dataset:** Netflix titles dataset  

### **Day 3: Housing Market Trends Prediction** ğŸ 
**ğŸ“ Folder:** `Day 3/`  
**ğŸ““ Notebook:** `3. Predicting Housing Market Trends with AI.ipynb`  
**ğŸ¯ Objective:** Build regression models to predict house sale prices using advanced ML techniques  
**ğŸ”§ Skills:** Regression modeling, feature engineering, XGBoost, model evaluation  
**ğŸ“Š Dataset:** Housing market data  

### **Day 4: Heart Disease Prediction** ğŸ©º
**ğŸ“ Folder:** `Day 4/`  
**ğŸ““ Notebook:** `4_AI_in_Healthcare_Building_a_Life_Saving_Heart_Disease_Predictor.ipynb`  
**ğŸ¯ Objective:** Build classification models to predict heart disease in patients  
**ğŸ”§ Skills:** Binary classification, medical data analysis, model comparison  
**ğŸ“Š Dataset:** Heart disease medical records  

### **Day 5: Customer Segmentation with AI** ğŸ›ï¸
**ğŸ“ Folder:** `Day 5/`  
**ğŸ““ Notebook:** `5_Smart_Segmentation_Unlocking_Customer_Personas_with_AI.ipynb`  
**ğŸ¯ Objective:** Use unsupervised learning (K-Means clustering) to identify customer segments  
**ğŸ”§ Skills:** Unsupervised learning, clustering, customer analytics  
**ğŸ“Š Dataset:** Mall customers dataset  

### **Day 6: Store Sales Forecasting** ğŸ“ˆ
**ğŸ“ Folder:** `Day 6/`  
**ğŸ““ Notebook:** `6_Predicting_Future_Store_Sales_with_AI.ipynb`  
**ğŸ¯ Objective:** Time series analysis and forecasting for retail store sales  
**ğŸ”§ Skills:** Time series analysis, ARIMA/SARIMA, feature engineering, forecasting  
**ğŸ“Š Dataset:** Retail store sales time series data  

### **Day 7: Customer Churn Prevention** ğŸ“¡
**ğŸ“ Folder:** `Day 7/`  
**ğŸ““ Notebook:** `7_Preventing_Customer_Churn_with_Feature_Transformation.ipynb`  
**ğŸ¯ Objective:** Advanced feature engineering to predict customer churn in telecom  
**ğŸ”§ Skills:** Feature engineering, data transformation, churn prediction  
**ğŸ“Š Dataset:** Telecom customer churn data  

### **Day 8: CIFAR-100 Image Classification** ğŸ–¼ï¸
**ğŸ“ Folder:** `Day 8/`  
**ğŸ““ Notebook:** `8_CIFAR_100_Image_Classification_from_Scratch.ipynb`  
**ğŸ¯ Objective:** Build CNN models from scratch for 100-class image classification  
**ğŸ”§ Skills:** Deep learning, computer vision, CNN architecture design  
**ğŸ“Š Dataset:** CIFAR-100 dataset (60,000 color images)  

### **Day 9: Transfer Learning for Image Classification** ğŸ”„
**ğŸ“ Folder:** `Day 9/`  
**ğŸ““ Notebook:** `Image_Classification_with_Transfer_Learning.ipynb`  
**ğŸ¯ Objective:** Apply transfer learning using pre-trained models (ResNet50, VGG16, MobileNetV2)  
**ğŸ”§ Skills:** Transfer learning, pre-trained models, fine-tuning  
**ğŸ“Š Dataset:** Oxford Flowers 102 dataset  

### **Day 10: Neural Style Transfer** ğŸ¨
**ğŸ“ Folder:** `Day 10/`  
**ğŸ““ Notebook:** `Generating_Art_with_Neural_Style_Transfer.ipynb`  
**ğŸ¯ Objective:** Generate artistic images using neural style transfer techniques  
**ğŸ”§ Skills:** Deep learning, computer vision, artistic AI applications  
**ğŸ“Š Dataset:** Custom face generation and style transfer  

### **Day 11: Hugging Face Pipelines** ğŸ¤—
**ğŸ“ Folder:** `Day 11/`  
**ğŸ““ Notebook:** `11_The_AI_Swiss_Army_Knife__One_Line_Solutions_with_Hugging_Face_Pipelines.ipynb`  
**ğŸ¯ Objective:** Master Hugging Face transformers for NLP and CV tasks  
**ğŸ”§ Skills:** NLP pipelines, sentiment analysis, text generation, image classification  
**ğŸ“Š Dataset:** Various text and image datasets  

### **Day 12: Face Resolution Enhancement with U-Net** ğŸ‘¤ğŸ”§
**ğŸ“ Folder:** `Day 12/`  
**ğŸ““ Notebook:** `face-resolution-enhancement-with-unet.ipynb`  
**ğŸ¯ Objective:** Enhance low-resolution face images using a U-Net based super-resolution pipeline  
**ğŸ”§ Skills:** Image super-resolution, image restoration, U-Net architecture, computer vision  
**ğŸ“Š Dataset:** Face image datasets (low/high resolution pairs)  

### **Day 13: Stock Price Prediction** ğŸ“Š
**ğŸ“ Folder:** `Day 13/`  
**ğŸ““ Notebook:** `Stock_Price_Prediction_NIfty_50.ipynb`  
**ğŸ¯ Objective:** Predict NIFTY 50 stock prices using ML and DL approaches  
**ğŸ”§ Skills:** Time series prediction, financial modeling, LSTM networks  
**ğŸ“Š Dataset:** NIFTY 50 stock price data  

### **Day 14: Code-Focused GPT-2 Inference with Filtering** ğŸ¤–
**ğŸ“ Folder:** `Day 14/`  
**ğŸ““ Notebook:** `Code_Focused_GPT_2_Inference_with_Filtering.ipynb`  
**ğŸ¯ Objective:** Run GPT-2 inference tailored for code with filtering to improve output quality  
**ğŸ”§ Skills:** Prompting, decoding strategies, output filtering, transformer inference  
**ğŸ“Š Dataset:** Code/text prompts  

### **Day 15: Natural Language to SQL** ğŸ’¬
**ğŸ“ Folder:** `Day 15/`  
**ğŸ““ Notebook:** `15_Talk_to_Your_Data_Building_a_Natural_Language_to_SQL_Generator.ipynb`  
**ğŸ¯ Objective:** Build a system that converts natural language queries to SQL  
**ğŸ”§ Skills:** NLP, SQL generation, database interaction  
**ğŸ“Š Dataset:** Custom database schemas and queries  

### **Day 16: Intelligent OCR Bot** ğŸ“„
**ğŸ“ Folder:** `Day 16/`  
**ğŸ““ Notebook:** `16_Intelligent_Document_Automation_Building_a_Smart_OCR_Bot_1.ipynb`  
**ğŸ¯ Objective:** Build an intelligent OCR system for document processing  
**ğŸ”§ Skills:** OCR, document processing, computer vision  
**ğŸ“Š Dataset:** Various document types and formats  

### **Day 17: Intelligent Search Engine** ğŸ”
**ğŸ“ Folder:** `Day 17/`  
**ğŸ““ Notebook:** `17_Build_Your_Own_Intelligent_Internet_Search_Engine.ipynb`  
**ğŸ¯ Objective:** Build an AI-powered web crawler and search engine  
**ğŸ”§ Skills:** Web scraping, search algorithms, AI agents  
**ğŸ“Š Dataset:** Web content and search queries

### **Day 18: RAG Chatbot - Chat with Your Knowledge Base** ğŸ’¬ğŸ“š
**ğŸ“ Folder:** `Day 18/`  
**ğŸ““ Notebook:** `18_Chat_with_Your_Knowledge_Base_Building_a_Powerful_RAG_Chatbot.ipynb`  
**ğŸ¯ Objective:** Build a Retrieval-Augmented Generation (RAG) chatbot over custom documents  
**ğŸ”§ Skills:** Retrieval pipelines, embeddings, context augmentation, chat UX  
**ğŸ“Š Dataset:** Local knowledge base (e.g., PDFs/CSVs)  

### **Day 19: Autonomous Market Analyst with AI Agents** ğŸ“ˆğŸ¤–
**ğŸ“ Folder:** `Day 19/`  
**ğŸ““ Notebook:** `19_Autonomous_Market_Analyst_Building_AI_Agents_for_Deep_Research.ipynb`  
**ğŸ¯ Objective:** Automate deep-dive market research using autonomous agent workflows  
**ğŸ”§ Skills:** Agent orchestration, tool use, information synthesis  
**ğŸ“Š Dataset:** Web sources, reports, and articles  

### **Day 20: AI Browser Agent for Web Automation** ğŸŒğŸ§­
**ğŸ“ Folder:** `Day 20/`  
**ğŸ““ Notebook:** `20_Web_Automation_on_Autopilot_Building_an_AI_Browser_Agent.ipynb`  
**ğŸ¯ Objective:** Automate browsing tasks and data collection using an AI browser agent  
**ğŸ”§ Skills:** Web automation, navigation policies, scraping workflow design  
**ğŸ“Š Dataset:** Live web pages and histories  

### **Day 21: AI-Powered Newsletter Pipeline on n8n** ğŸ“¨âš™ï¸
**ğŸ“ Folder:** `Day 21/`  
**ğŸ““ Notebook:** `21_Building_an_AI_Powered_Newsletter_Pipeline_on_n8n (1).ipynb`  
**ğŸ¯ Objective:** Orchestrate an automated newsletter pipeline using n8n with AI components  
**ğŸ”§ Skills:** Workflow automation, integrations, content generation, scheduling  
**ğŸ“Š Dataset:** Curated links, summaries, and generated content  

---

## ğŸ› ï¸ Technologies Used

### **Programming Languages**
- **Python** - Primary language for all projects
- **SQL** - Database queries and data manipulation

### **Data Science & ML Libraries**
- **Pandas** - Data manipulation and analysis
- **NumPy** - Numerical computing
- **Scikit-learn** - Machine learning algorithms
- **XGBoost** - Gradient boosting framework
- **LightGBM** - Gradient boosting framework

### **Deep Learning Frameworks**
- **TensorFlow/Keras** - Neural network development
- **PyTorch** - Deep learning research and development
- **Transformers (Hugging Face)** - Pre-trained NLP models

### **Visualization Libraries**
- **Matplotlib** - Basic plotting and visualization
- **Seaborn** - Statistical data visualization
- **Plotly** - Interactive visualizations

### **Specialized Tools**
- **OpenCV** - Computer vision tasks
- **YOLO** - Object detection
- **Crawl4AI** - Web scraping and crawling
- **Statsmodels** - Statistical modeling
- **n8n** - Workflow automation

---

## ğŸ“Š Project Categories

| Category | Projects | Skills Developed |
|----------|----------|------------------|
| **ğŸ“Š Data Analysis** | Days 1-2 | EDA, Statistical Analysis, Visualization |
| **ğŸ¤– Machine Learning** | Days 3-7 | Classification, Regression, Clustering, Time Series |
| **ğŸ§  Deep Learning** | Days 8-10 | CNNs, Transfer Learning, Style Transfer |
| **ğŸ”§ AI Applications** | Days 11-13 | NLP, Computer Vision, Financial Modeling |
| **ğŸš€ Advanced AI** | Days 14-21 | Language Models, OCR, Search Engines, RAG, Agents, Automation |

---

## ğŸš€ Getting Started

### **Prerequisites**
- Python 3.7+
- Jupyter Notebook or JupyterLab
- Basic understanding of Python programming
- Familiarity with data science concepts (recommended)

### **Installation**
1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/21-days-of-project.git
   cd 21-days-of-project
   ```

2. **Install required packages:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Launch Jupyter Notebook:**
   ```bash
   jupyter notebook
   ```

### **Running Projects**
1. Navigate to the desired day's folder
2. Open the corresponding Jupyter notebook
3. Follow the step-by-step instructions
4. Execute cells in order for best results

---

---

## ğŸ¤ Contributing

We welcome contributions to improve this project! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch:** `git checkout -b feature/amazing-feature`
3. **Commit your changes:** `git commit -m 'Add some amazing feature'`
4. **Push to the branch:** `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### **Contribution Guidelines**
- Follow the existing code style and documentation format
- Add clear comments and docstrings to your code
- Include test cases for new functionality
- Update the README if you add new features

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸŒŸ Acknowledgments

- **Dataset Providers:** Kaggle, Hugging Face, and various open-source data providers
- **Open Source Libraries:** All the amazing Python libraries that make these projects possible
- **Community:** The data science and AI community for inspiration and support

---

## ğŸ“ Contact & Support

- **Issues:** [GitHub Issues](https://github.com/yourusername/21-days-of-project/issues)
- **Discussions:** [GitHub Discussions](https://github.com/yourusername/21-days-of-project/discussions)
- **Email:** your.email@example.com

---

<div align="center">

**â­ Star this repository if you found it helpful!**

**ğŸš€ Happy Learning and Building!**

</div>
